<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>人工智能 | 风之谷</title>
    <meta name="generator" content="VuePress 1.9.9">
    <link rel="shortcut icon" type="image/x-icon" href="./favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css">
    <meta name="description" content="技术博客">
    
    <link rel="preload" href="/assets/css/0.styles.82ff4c1c.css" as="style"><link rel="preload" href="/assets/js/app.6de03009.js" as="script"><link rel="preload" href="/assets/js/6.f588e3a4.js" as="script"><link rel="preload" href="/assets/js/11.20194e5d.js" as="script"><link rel="preload" href="/assets/js/4.33f369b0.js" as="script"><link rel="prefetch" href="/assets/js/10.04fcc694.js"><link rel="prefetch" href="/assets/js/12.7245c36e.js"><link rel="prefetch" href="/assets/js/13.a4676484.js"><link rel="prefetch" href="/assets/js/14.aa96bfbd.js"><link rel="prefetch" href="/assets/js/15.62aaec84.js"><link rel="prefetch" href="/assets/js/16.b747235b.js"><link rel="prefetch" href="/assets/js/17.44b879c8.js"><link rel="prefetch" href="/assets/js/18.83909f3e.js"><link rel="prefetch" href="/assets/js/19.821915b5.js"><link rel="prefetch" href="/assets/js/20.d9c908bb.js"><link rel="prefetch" href="/assets/js/21.ff516ada.js"><link rel="prefetch" href="/assets/js/22.6e86c9ad.js"><link rel="prefetch" href="/assets/js/23.92ccdfb1.js"><link rel="prefetch" href="/assets/js/24.8d84a181.js"><link rel="prefetch" href="/assets/js/25.77182731.js"><link rel="prefetch" href="/assets/js/26.0d2858e0.js"><link rel="prefetch" href="/assets/js/27.1ea6497e.js"><link rel="prefetch" href="/assets/js/28.64e27cd6.js"><link rel="prefetch" href="/assets/js/29.330fd6d6.js"><link rel="prefetch" href="/assets/js/3.46d2affe.js"><link rel="prefetch" href="/assets/js/30.82947b5d.js"><link rel="prefetch" href="/assets/js/31.60edb649.js"><link rel="prefetch" href="/assets/js/32.0cead929.js"><link rel="prefetch" href="/assets/js/33.2d7a85dc.js"><link rel="prefetch" href="/assets/js/34.1f616a83.js"><link rel="prefetch" href="/assets/js/35.8955338e.js"><link rel="prefetch" href="/assets/js/36.7afe7489.js"><link rel="prefetch" href="/assets/js/37.ee3cfa08.js"><link rel="prefetch" href="/assets/js/38.ebbcde82.js"><link rel="prefetch" href="/assets/js/39.dc342631.js"><link rel="prefetch" href="/assets/js/5.db94fb56.js"><link rel="prefetch" href="/assets/js/7.2b7ee228.js"><link rel="prefetch" href="/assets/js/8.da88cc7c.js"><link rel="prefetch" href="/assets/js/9.3cb34b32.js"><link rel="prefetch" href="/assets/js/vuejs-paginate.ac365ac7.js">
    <link rel="stylesheet" href="/assets/css/0.styles.82ff4c1c.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div id="vuepress-theme-blog__global-layout"><section id="header-wrapper"><header id="header"><div class="header-wrapper"><div class="title"><a href="/" class="nav-link home-link">风之谷 </a></div> <div class="header-right-wrap"><ul class="nav"><li class="nav-item"><a href="/ai/" aria-current="page" class="nav-link router-link-exact-active router-link-active">AI</a></li><li class="nav-item"><a href="/leetcode/" class="nav-link">LeetCode</a></li><li class="nav-item"><a href="/devtools/" class="nav-link">DevTools</a></li><li class="nav-item"><a href="/tag/" class="nav-link">标签</a></li></ul> <div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></div></header></section> <div id="mobile-header"><div class="mobile-header-bar"><div class="mobile-header-title"><a href="/" class="nav-link mobile-home-link">风之谷 </a> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></div> <div class="mobile-menu-wrapper"><hr class="menu-divider"> <ul class="mobile-nav"><li class="mobile-nav-item"><a href="/ai/" aria-current="page" class="nav-link router-link-exact-active router-link-active">AI</a></li><li class="mobile-nav-item"><a href="/leetcode/" class="nav-link">LeetCode</a></li><li class="mobile-nav-item"><a href="/devtools/" class="nav-link">DevTools</a></li><li class="mobile-nav-item"><a href="/tag/" class="nav-link">标签</a></li> <li class="mobile-nav-item"><!----></li></ul></div></div></div> <div class="content-wrapper"><div><div id="base-list-layout"><div itemscope="itemscope" itemtype="http://schema.org/Blog" class="ui-posts"><article itemprop="blogPost" itemscope="itemscope" itemtype="https://schema.org/BlogPosting" class="ui-post"><meta itemprop="mainEntityOfPage" content="/ai/2024/05/11/flashattention/"> <header itemprop="name headline" class="ui-post-title"><a href="/ai/2024/05/11/flashattention/" class="nav-link">FlashAttention 1&amp;2</a></header> <p itemprop="description" class="ui-post-summary">
        

在学习大模型训练库unsloth时看到和FlashAttention的对比，因为之前也听说过，感觉这个技术还挺火的，所以趁这个机会研究一下。

性能

根据官方提供的数据，和原始Pytorch版本相比，FlashAttention的速度至少提升了2倍，FlashAttention2的速度至少提升了3倍。内存减少量随着序列长度线性增长，当序列长度为2k时，内存减少了10倍，当序列长度为4k ...
      </p> <footer><!----> <div class="ui-post-meta ui-post-date"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg> <time pubdate itemprop="datePublished" datetime="2024-05-11T00:00:00.000Z">
            Sat May 11 2024
          </time></div> <div itemprop="keywords" class="ui-post-meta ui-post-tag"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg> <a href="/tag/ai">
            ai
          </a><a href="/tag/llm">
            llm
          </a><a href="/tag/attention">
            attention
          </a></div></footer></article><article itemprop="blogPost" itemscope="itemscope" itemtype="https://schema.org/BlogPosting" class="ui-post"><meta itemprop="mainEntityOfPage" content="/ai/2024/04/21/direct-preference-optimization/"> <header itemprop="name headline" class="ui-post-title"><a href="/ai/2024/04/21/direct-preference-optimization/" class="nav-link">DPO(Direct Preference Optimization)：LLM的直接偏好优化</a></header> <p itemprop="description" class="ui-post-summary">
        

在学习llama模型的训练过程中发现强化学习除了PPO(proximal policy optimization)还有一个DPO可选项(direct preference optimization)，在我的上一篇笔记里有提到两本入门课程（Easy RL和动手学强化学习）里并没有提到这个算法，应该是最近新出现的，查了一下该算法是在23年提出的。

在学习DPO之前，我们先回顾一下RLHF。
 ...
      </p> <footer><!----> <div class="ui-post-meta ui-post-date"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg> <time pubdate itemprop="datePublished" datetime="2024-04-21T00:00:00.000Z">
            Sun Apr 21 2024
          </time></div> <div itemprop="keywords" class="ui-post-meta ui-post-tag"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg> <a href="/tag/ai">
            ai
          </a><a href="/tag/reinforce learning">
            reinforce learning
          </a><a href="/tag/direct preference optimization">
            direct preference optimization
          </a></div></footer></article><article itemprop="blogPost" itemscope="itemscope" itemtype="https://schema.org/BlogPosting" class="ui-post"><meta itemprop="mainEntityOfPage" content="/ai/2024/04/11/qlora-efficient-finetuning-of-quantized-llms/"> <header itemprop="name headline" class="ui-post-title"><a href="/ai/2024/04/11/qlora-efficient-finetuning-of-quantized-llms/" class="nav-link">QLORA：量化LLM的高效微调</a></header> <p itemprop="description" class="ui-post-summary">
        

一般来说我们平时训练时使用的是32位浮点数（以下简称FP32），但是FP32占用内存较高，如果你的显卡的显存不够大就无法训练了，这时候可以用到量化（Quantization），将FP32压缩到FP16或者FP8以减少内存占用。QLoRA是目前比较流行的量化微调技术，它由微软在23年提出。在阅读了其相关的资料后，接下来分析一下它的核心算法。

LoRA

在分析QLoRA之前，先介绍一下它 ...
      </p> <footer><!----> <div class="ui-post-meta ui-post-date"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg> <time pubdate itemprop="datePublished" datetime="2024-04-11T00:00:00.000Z">
            Thu Apr 11 2024
          </time></div> <div itemprop="keywords" class="ui-post-meta ui-post-tag"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg> <a href="/tag/ai">
            ai
          </a><a href="/tag/qlora">
            qlora
          </a><a href="/tag/quantization">
            quantization
          </a></div></footer></article><article itemprop="blogPost" itemscope="itemscope" itemtype="https://schema.org/BlogPosting" class="ui-post"><meta itemprop="mainEntityOfPage" content="/ai/2024/03/26/proximal-policy-optimization/"> <header itemprop="name headline" class="ui-post-title"><a href="/ai/2024/03/26/proximal-policy-optimization/" class="nav-link">近端策略优化</a></header> <p itemprop="description" class="ui-post-summary">
        

上一篇文章学习了策略梯度，它是同策略，采样学习过一次的数据在执行梯度上升之后就不能再用了，需要重新采样，所以它在采样上花费了大量的时间。而近端策略优化解决了这个问题，它是策略梯度的变种，也是异策略，它用另外一个策略和演员同环境交互，让原来的策略去学习另外一个策略，这样可以多次使用另外一个策略采样到的数据，可以多次执行梯度上升。

关键词

同策略和异策略：如果要学习的智能体和与环境交 ...
      </p> <footer><!----> <div class="ui-post-meta ui-post-date"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg> <time pubdate itemprop="datePublished" datetime="2024-03-26T00:00:00.000Z">
            Tue Mar 26 2024
          </time></div> <div itemprop="keywords" class="ui-post-meta ui-post-tag"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg> <a href="/tag/ai">
            ai
          </a><a href="/tag/reinforce learning">
            reinforce learning
          </a><a href="/tag/proximal policy optimization">
            proximal policy optimization
          </a></div></footer></article><article itemprop="blogPost" itemscope="itemscope" itemtype="https://schema.org/BlogPosting" class="ui-post"><meta itemprop="mainEntityOfPage" content="/ai/2024/03/07/policy-gradient/"> <header itemprop="name headline" class="ui-post-title"><a href="/ai/2024/03/07/policy-gradient/" class="nav-link">策略梯度</a></header> <p itemprop="description" class="ui-post-summary">
        

在学习大模型的训练过程中发现在微调之后需要用到强化学习，常见的方法如近端策略优化（PPO）。在查询资料后发现在没有系统性学习强化学习的基础知识的前提下想直接学习PPO会非常困难。于是我决定先放下大模型训练的学习，转而学习强化学习。

在查询了相关教程后，发现了两本比较适合入门的书籍，都有配套的在线教程。分别是[Easy RL 强化学习教程](https://datawhalechina.git ...
      </p> <footer><!----> <div class="ui-post-meta ui-post-date"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg> <time pubdate itemprop="datePublished" datetime="2024-03-07T00:00:00.000Z">
            Thu Mar 07 2024
          </time></div> <div itemprop="keywords" class="ui-post-meta ui-post-tag"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg> <a href="/tag/ai">
            ai
          </a><a href="/tag/reinforce learning">
            reinforce learning
          </a><a href="/tag/policy gradient">
            policy gradient
          </a></div></footer></article></div> <!----></div></div></div> <footer class="footer" data-v-3d9deeb8><div class="footer-left-wrap" data-v-3d9deeb8><ul class="contact" data-v-3d9deeb8></ul></div> <div class="footer-right-wrap" data-v-3d9deeb8><ul class="copyright" data-v-3d9deeb8></ul></div></footer></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.6de03009.js" defer></script><script src="/assets/js/6.f588e3a4.js" defer></script><script src="/assets/js/11.20194e5d.js" defer></script><script src="/assets/js/4.33f369b0.js" defer></script>
  </body>
</html>
